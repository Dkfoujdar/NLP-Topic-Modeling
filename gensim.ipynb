{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Packages\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('stopwords')\n",
    "from gensim.corpora import Dictionary\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel, HdpModel\n",
    "#Packages for the visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## using functions to create bigram ,trigram, adjuctive and noun "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre Processing:\n",
    "### Using Spacy and nltk for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "def Activity_norm(text):\n",
    "    if text == 'DELIVERY PARTIAL' or text=='BOTH DELIVERY PICK UP PARTIAL' or text=='OTHER SERVICESPartial' or text=='RELOCATION PARTIAL': \n",
    "        return('PARTIAL')\n",
    "    elif text=='DELIVERY CANCELLED RESCHEDULED' or text=='BOTH DELIVERY PICK UP CANCELLED RESCHEDULED' or text == 'RELOCATION CANCELLED RESCHEDULED' or text== 'OTHER SERVICESCancelled Re scheduled':\n",
    "        return('CANCELLED')\n",
    "    else:\n",
    "        return(text)\n",
    "def remove_stopwords(texts):\n",
    "    stop_words= stopwords.words(\"english\")\n",
    "    stop_words.extend(['please','other','reason','unit,','pm','today','tomorrow','plz'])\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigram(texts):\n",
    "    return [Bigram_mode[doc] for doc in texts]\n",
    "def make_trigram(texts):\n",
    "    return [Trigram_mod[doc] for doc in texts]\n",
    "def Lemmatize(texts,allowed_postags=['NOUN','VERB','ADJ','ADV']):\n",
    "    text_out=[]\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        text_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return text_out\n",
    "def compute_coherence_values(id2word, cropus,lemma_data, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=lemma_data, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Import'\n",
    "import warnings\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "    df=pd.read_excel(\"Feedbaacks_file_1.xlsx\")\n",
    "    df1=pd.read_excel(\"Feedbaacks_file_2.xlsx\", sheet_name='DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Merging/..\n",
    "Comment=pd.concat([df['CRM_Remarks'],df1['CEM Reasons']])\n",
    "Activity=pd.concat([df['ACTIVITY_STATUS_TYPE'],df1['Activity Type']])\n",
    "data=pd.DataFrame({\"Comment\":Comment,\"Activity\":Activity})\n",
    "data=data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Pre Processing \n",
    "# This section is your data specific and \n",
    "datax=data\n",
    "datax['Activity']=datax['Activity'].map(lambda text: \" \".join(re.findall('\\w+',str(text))))\n",
    "datax['Activity']=datax['Activity'].map(lambda text: Activity_norm(text))\n",
    "datax['Activity']=datax['Activity'].map(lambda text: text.lower())\n",
    "datax=datax[datax['Activity']!=3]\n",
    "datax=datax[datax['Activity']!='completed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Pre-Processing\n",
    "datax['Comment']=datax['Comment'].apply(lambda text: re.sub(r'\\(.*\\)','',str(text))) #deleting the values inside the bracket\n",
    "data_words=list(sent_to_words(datax[\"Comment\"])) #using the self made function\n",
    "Bigram=gensim.models.Phrases(data_words, min_count=5,threshold=100)\n",
    "Trigram=gensim.models.Phrases(Bigram[data_words],min_count=5,threshold=100)\n",
    "Bigram_mode=gensim.models.phrases.Phraser(Bigram)\n",
    "Trigram_mod=gensim.models.phrases.Phraser(Trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text pre-processing\n",
    "\n",
    "#removing the stopwords \n",
    "data_words_nonstop=remove_stopwords(data_words)\n",
    "\n",
    "#making bigrams\n",
    "bigram_data=make_bigram(data_words_nonstop)\n",
    "\n",
    "#making trigrams\n",
    "#rigram_data=make_trigram(bigram_data)\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "#lemmatization\n",
    "lemma_data=Lemmatize(bigram_data, allowed_postags=['NOUN','VERB','ADJ','ADV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For testing the frequncy of the different word\n",
    "cv=CountVectorizer(max_df=0.95, min_df=50, stop_words=\"english\")\n",
    "x=cv.fit_transform([' '.join(text) for text in lemma_data])\n",
    "counter=pd.DataFrame(x.todense(),columns=cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency of the top 100 words\n",
    "sum_words = x.sum(axis=0)\n",
    "words_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\n",
    "words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "Top_words=words_freq[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Top_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim lda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dictionary and corpus\n",
    "id2word = corpora.Dictionary(lemma_data)\n",
    "texts=lemma_data\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=8,\n",
    "                                           random_state=111,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "print(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extraxting only topics from the matrix \n",
    "topics=' //Topic\\\\: '.join([' '.join([''.join(tup[0]) for tup in sent[1]]) for sent in lda_model.show_topics(num_topics=20,formatted =False)])\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn lda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Tried for hours  but  Unable to run the code for lda mallet. First error was the filenotfounderror and the calledProcessError\n",
    "#mallet_path= \"/bin/mallet\"\n",
    "#ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corp, num_topics=15, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skl_texts=[' '.join(text) for text in lemma_data]\n",
    "no_features = 1000\n",
    "# NMF(Non negetive Matrix Factorization ) is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(skl_texts)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(skl_texts)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_topics = 20\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation(n_topics=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n",
    "\n",
    "no_top_words = 10\n",
    "#display_topics(nmf, tfidf_feature_names, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sklearn Hdp model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hdp = HdpModel(corpus=corpus, id2word=id2word)\n",
    "#[' '.join([''.join(tup[0]) for tup in sent[1]]) for sent in hdp.show_topics(formatted =False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visuallization of the results\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer)\n",
    "#vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=lemma_data, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(id2word, corpus, lemma_data, 40, 2, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
